---
title: 【强化学习大作业】双升游戏 AI
date: 2026-01-18
summary: 用 Transformer 训练游戏 AI
authors:
  - admin
categories:
  - ri-ji
tags:
  - RL
  - Transformer
---

学期末终于把这个大作业搞定了，简单记录一下。

- [GitHub](https://github.com/keyijing/Tractor-RL)
- [report](report.pdf)

---

课程作业里训练的都是几个全连接层的小模型，这次大作业我心血来潮决定用当前最流行的 Transformer，来训练一个[双升](https://botzone.org.cn/game/Tractor)游戏的 Agent。不知道能不能训得动……🤔

## 状态与动作表征

核心思路是将状态和动作都表示为一个或多个 token，生成动作时只用 next token prediction 就行了。状态表征的思路很简单：按照时间顺序，把 Agent 观察到的信息一个个拼接起来。每种本质上不同的信息对应不同的 token（比如拿到什么牌、看到其他人出了什么牌、自己出了什么牌）。

动作的表征也采用同样的方法，用不同的 token 表示不同的动作。如果要一次性打出多张牌，就用多个 token 表示这一个动作。

为了让模型能自回归地生成 token，得把动作的 token 也拼接到状态后面。这样就能保证：一段轨迹的某一个前缀的表征，恰好是整个轨迹表征的前缀。

## 计算合法动作

如何让 Agent 只做符合规则的动作？这是写起来最头疼的一部分。由于一个动作可能由多个 token 表示，所以问题变成：给定已生成的部分 token，接下来哪些 token 是合法的。

这里我偷了点懒😅，只允许一部分合法的 token 序列出现（比如必须按第一名玩家牌型的固定顺序出牌）。这样一来，判断能否打出一张牌就可以贪心地搞定。

由于该算法比较暴力，为了不影响采样速度，就用 C++ 实现了这个模块，然后绑定到 Python 里调用。

## 训练：出乎意料的顺利

代码写完后，居然没怎么调试就开始稳定训练了！据说使用 CNN 模型训练的小组，如果只用自对弈，反而一直训不起来。

琢磨了一下，感觉 Transformer 在这种任务上反而比传统架构更适合并行化训练：一整局游戏的所有 token 可以拼成序列一起输入 Transformer。假设 batch size 是 $128$，一局游戏有 $50$ 个动作，那么每个 batch 就包含了 $128\times 50=6400$ 个状态-动作对。

而传统架构一次只能输入一个状态。要想一个 batch 训练这么多样本，得把 batch size 开到四位数，这样显存直接爆了。💥

---

最后居然拿了第一！🎉 看来没白折腾，开心。😎
